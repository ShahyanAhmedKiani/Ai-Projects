# -*- coding: utf-8 -*-
"""Multi_Emotion_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vsyc3GXPGTsVFbXwT328qpjsK90QbDEO

-------------------------------------------
# ✅ MULTI EMOTION
-------------------------------------------
"""

!pip install transformers datasets scikit-learn torch --quiet

"""-------------------------------------------
# ✅ Import Libraries
-------------------------------------------
"""

import torch
from datasets import load_dataset
from transformers import BertTokenizerFast
from sklearn.preprocessing import MultiLabelBinarizer
from transformers import BertForSequenceClassification, Trainer, TrainingArguments
import os

"""-------------------------------------------
# ✅ Load DATASET
-------------------------------------------
"""

# Load the GoEmotions dataset
dataset = load_dataset("go_emotions")

# Get label names and initialize tokenizer
label_names = dataset['train'].features['labels'].feature.names
num_labels = len(label_names)
tokenizer = BertTokenizerFast.from_pretrained("bert-base-uncased")

"""-------------------------------------------
# ✅ Preprocess
-------------------------------------------
"""

# ✅ Preprocessing: convert labels to float values
def preprocess(example):
    encoding = tokenizer(example["text"], padding="max_length", truncation=True, max_length=128)
    labels = [0.0] * num_labels   # Use float
    for label in example['labels']:
        labels[label] = 1.0
    encoding["labels"] = labels
    return encoding

# Apply preprocessing
encoded_dataset = dataset.map(preprocess, batched=False)
encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])

"""-------------------------------------------
# ✅ Model and Training Setup
-------------------------------------------

"""

# Disable wandb logging

os.environ["WANDB_DISABLED"] = "true"

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=num_labels,
    problem_type="multi_label_classification"
)

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=2,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=10,
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=encoded_dataset["train"].shuffle(seed=42).select(range(5000)),
    eval_dataset=encoded_dataset["validation"].select(range(1000)),
)

# ✅ Fix label type inside Trainer
def compute_loss(model, inputs, return_outputs=False,num_items_in_batch=None):
    inputs["labels"] = inputs["labels"].type(torch.float32)
    outputs = model(**inputs)
    loss_fct = torch.nn.BCEWithLogitsLoss()
    loss = loss_fct(outputs.logits, inputs["labels"])
    return (loss, outputs) if return_outputs else loss

trainer.compute_loss = compute_loss
trainer.train()

"""-------------------------------------------
# ✅ Metrics
-------------------------------------------
"""

from sklearn.metrics import f1_score, hamming_loss

def compute_metrics(pred):
    logits, labels = pred
    probs = torch.sigmoid(torch.tensor(logits))
    preds = (probs > 0.3).int()
    labels = torch.tensor(labels)
    return {
        "micro_f1": f1_score(labels, preds, average='micro'),
        "hamming_loss": hamming_loss(labels, preds)
    }

trainer.compute_metrics = compute_metrics
trainer.evaluate()

"""-------------------------------------------
#✅ Predict emotions from custom text
-------------------------------------------
"""

def predict_emotions(text):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()  # <-- Ensure eval mode

    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.sigmoid(outputs.logits)
        preds = (probs > 0.2).int().squeeze().tolist()

    return [label_names[i] for i, p in enumerate(preds) if p == 1]

# ✅ Example usage
test_text = "I'm feeling so excited and thankful for everything today!"
print("Predicted Emotions:", predict_emotions(test_text))

from transformers import pipeline
classifier = pipeline("text-classification", model="j-hartmann/emotion-english-distilroberta-base", top_k=None)
classifier("I'm feeling so excited and thankful for everything today!")

# from transformers import BertTokenizerFast

# # Save model and tokenizer
# model_path = "./goemotions_bert"
# model.save_pretrained(model_path)
# tokenizer.save_pretrained(model_path)

# # Zip it for download
# import shutil
# shutil.make_archive("goemotions_bert", 'zip', model_path)

# # Download
# from google.colab import files
# files.download("goemotions_bert.zip")